{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MJVx26HrrBL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.transforms import ToPILImage, Grayscale"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMNISTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transform = None, train=True):\n",
        "        self.mnist_data = datasets.MNIST(root=root, train=train, transform=transform, download=True)\n",
        "        self.train = train\n",
        "        self.resize_transform = transforms.Resize((224, 224))\n",
        "\n",
        "        if train:\n",
        "            self.filtered_indices = [idx for idx, label in enumerate(self.mnist_data.targets) if label < 6]\n",
        "        else:\n",
        "            self.filtered_indices = [6 if label >= 6 else label for _, label in self.mnist_data]\n",
        "        #self.transform = transform\n",
        "        #self.to_pil = ToPILImage()\n",
        "        #self.grayscale = Grayscale(num_output_channels=3)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filtered_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.train:\n",
        "            filtered_idx = self.filtered_indices[idx]\n",
        "            image, label = self.mnist_data[filtered_idx]\n",
        "        else:\n",
        "            image, _ = self.mnist_data[idx]\n",
        "            label = self.filtered_indices[idx]\n",
        "\n",
        "\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "s26OsRlLr5j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformations\n",
        "#transform = transforms.Compose([transforms.ToTensor()])\n",
        "transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Lambda(lambda x: torch.cat([x, x, x], 0)),\n",
        "                #transforms.Normalize(mean, std),\n",
        "                 ])\n",
        "## train_test dataset\n",
        "train_dataset = CustomMNISTDataset(root='path_to_mnist_dataset12', transform=transform, train=True)\n",
        "test_dataset = CustomMNISTDataset(root='path_to_mnist_dataset12', transform=transform, train=False)\n",
        "\n",
        "\n",
        "## Train_Testest Dataloader\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "6-UhvPS6r_mc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e4412d-0309-45be-a368-e8a17f2ca3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to path_to_mnist_dataset12/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 240618282.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting path_to_mnist_dataset12/MNIST/raw/train-images-idx3-ubyte.gz to path_to_mnist_dataset12/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to path_to_mnist_dataset12/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 92682244.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting path_to_mnist_dataset12/MNIST/raw/train-labels-idx1-ubyte.gz to path_to_mnist_dataset12/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to path_to_mnist_dataset12/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 73635197.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting path_to_mnist_dataset12/MNIST/raw/t10k-images-idx3-ubyte.gz to path_to_mnist_dataset12/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to path_to_mnist_dataset12/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 25919086.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting path_to_mnist_dataset12/MNIST/raw/t10k-labels-idx1-ubyte.gz to path_to_mnist_dataset12/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import optimizer\n",
        "from torch.distributions import Beta\n",
        "import tempfile\n",
        "import random\n",
        "import os\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "TFj0jZkBsIu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(myModel,self).__init__()\n",
        "    self.feat_extractor = models.resnet18(weights = 'IMAGENET1K_V1')\n",
        "    self.feat_extractor.fc = nn.Identity()\n",
        "    self.classifier = nn.Linear(512,7)\n",
        "  def forward(self,img,label):\n",
        "    feat = self.feat_extractor(img)\n",
        "    feat_new=torch.zeros_like(feat).to(device)\n",
        "    for i in range(img.shape[0]):\n",
        "        #for j in range(5):\n",
        "        lambda_ = Beta(torch.FloatTensor([2]), torch.FloatTensor([2])).sample()\n",
        "        lambda_ = lambda_.to(device)\n",
        "        # batch_j =[]\n",
        "        index1 = random.randint(0, img.shape[0] -1)\n",
        "        index2 = random.randint(0, img.shape[0] - 1)\n",
        "        # batch_j.append(img[index1])\n",
        "        # batch_j.append(img[index2])\n",
        "        mixup = lambda_ * feat[index1] + (1 - lambda_) * feat[index2]\n",
        "        # print(mixup.shape)\n",
        "        #x_midlayer = myModel().feat_extractor.fc(mixup)\n",
        "        #x_new = myModel().classifier(x_midlayer)\n",
        "        feat_new[i,:] = mixup\n",
        "    ####\n",
        "    feat_final = torch.cat((feat,feat_new),axis=0)\n",
        "    prob = self.classifier(feat_final)\n",
        "    return prob\n",
        "#def generate_data_placeholder(model, batch_size, dataset):\n",
        "    #mixed_outputs = []\n",
        "    #for _ in range(batch_size):\n",
        "        #lambda_ = Beta(torch.FloatTensor([2]), torch.FloatTensor([2])).sample()\n",
        "       # #index1 = random.randint(0, len(dataset) - 1)\n",
        "        #index2 = random.randint(0, len(dataset) - 1)\n",
        "        #img1, _ = dataset[index1]\n",
        "        #img2, _ = dataset[index2]\n",
        "        #img1 = img1.unsqueeze(0)\n",
        "        #img2 = img2.unsqueeze(0)\n",
        "        #mixup = lambda_ * model.feat_extractor(img1) + (1 - lambda_) * model.feat_extractor(img2)\n",
        "        #x_midlayer = model.feat_extractor.fc(mixup)\n",
        "        #x_new = model.classifier(x_midlayer)\n",
        "        #mixed_outputs.append(x_new)\n",
        "   # mixed_outputs = torch.cat(mixed_outputs, dim=0)\n",
        "   # return mixed_outputs"
      ],
      "metadata": {
        "id": "L5rh1DAUsDgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lLO7B2drsP4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = myModel().to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "dataloaders = {\n",
        "    'train': train_data_loader,\n",
        "    'val': test_data_loader\n",
        "}\n",
        "dataset_sizes = {\n",
        "    'train': len(train_data_loader),\n",
        "    'val':len(test_data_loader)\n",
        "}\n",
        "\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "GpQGOh9z_Xly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856e2b0e-3633-41b9-f79d-1f6cd5bf46e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 185MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "    # print(newlabel.shape)\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with tempfile.TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    newlabel = torch.ones(inputs.shape[0]) * 6\n",
        "                    # print(\"NewLable\",newlabel.shape)\n",
        "\n",
        "                    newlabel = newlabel.to(device)\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        # print(labels.shape)\n",
        "                        # print(newlabel.shape)\n",
        "                        label_update = torch.cat((labels,newlabel),axis = 0).type(torch.LongTensor)\n",
        "                        label_update=label_update.to(device)\n",
        "                        # print(label_update.shape)\n",
        "                        # print(inputs.shape)\n",
        "                        outputs = model(inputs,labels)\n",
        "                        # print(\"outputs\",outputs.shape)\n",
        "\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, label_update)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == label_update.data)\n",
        "                #if phase == 'train':\n",
        "                   # scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / (dataset_sizes[phase]*2*batch_size)\n",
        "                epoch_acc = running_corrects.double() / (dataset_sizes[phase]*2*batch_size)\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(torch.load(best_model_params_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "B3zI8vbIsYP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainresult = train_model(model, criterion, optimizer, scheduler, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al2wRTC8scum",
        "outputId": "b7b5edd0-a62f-4951-fe21-61c43cc03706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 0.2283 Acc: 0.8323\n",
            "val Loss: 0.2411 Acc: 0.7585\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "train Loss: 0.1683 Acc: 0.8787\n",
            "val Loss: 0.2719 Acc: 0.7159\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "train Loss: 0.1590 Acc: 0.8852\n",
            "val Loss: 0.2330 Acc: 0.7473\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "train Loss: 0.1530 Acc: 0.8878\n",
            "val Loss: 0.2488 Acc: 0.7250\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "train Loss: 0.1473 Acc: 0.8927\n",
            "val Loss: 0.2230 Acc: 0.7536\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "train Loss: 0.1470 Acc: 0.8929\n",
            "val Loss: 0.2535 Acc: 0.7195\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "train Loss: 0.1434 Acc: 0.8954\n",
            "val Loss: 0.2159 Acc: 0.7681\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n",
            "train Loss: 0.1422 Acc: 0.8952\n",
            "val Loss: 0.2615 Acc: 0.7036\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "train Loss: 0.1397 Acc: 0.8980\n",
            "val Loss: 0.2650 Acc: 0.7042\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n",
            "train Loss: 0.1406 Acc: 0.8962\n",
            "val Loss: 0.2414 Acc: 0.7342\n",
            "\n",
            "Training complete in 8m 20s\n",
            "Best val Acc: 0.768071\n"
          ]
        }
      ]
    }
  ]
}